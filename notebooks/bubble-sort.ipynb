{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable Bubble Sort\n",
    "\n",
    "Differentiable implementation of bubble sort with configurable (learnable) comparator function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swap Function\n",
    "\n",
    "Using linear interpolation for continious swap.\n",
    "\n",
    "\\begin{equation*}\n",
    "new_a = a * t + b * (1 - t)\n",
    "\\end{equation*}\n",
    "\\begin{equation*}\n",
    "new_b = b * t + a * (1 - t)\n",
    "\\end{equation*}\n",
    "\n",
    "When $t = 0$, then $a$ and $b$ are swapped. When $t = 1$, they remain in place.\n",
    "\n",
    "Other compare and swap strategies include [softmax approximation](https://github.com/johnhw/differentiable_sorting), [optimal transport](https://arxiv.org/pdf/1905.11885.pdf), [projecting into higher dimensional space](https://arxiv.org/pdf/2002.08871.pdf) etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def swap(x, i, j, t=None):\n",
    "    '''\n",
    "        Linear interpolation swap\n",
    "        x: Tensor - Expected dims: [2, feature_size]\n",
    "        i: Tensor - Scalar, int-like\n",
    "        j: Tensor - Scalar, int-like\n",
    "        t: Tensor - Scalar, float-like\n",
    "    '''\n",
    "    x_shape = tf.shape(x)\n",
    "    x_len = x_shape[0]\n",
    "    \n",
    "    if t is None:\n",
    "        t = tf.zeros(x_shape)\n",
    "    \n",
    "    i_pos_mask = tf.expand_dims(tf.eye(x_len)[i],axis=-1)\n",
    "    i_neg_mask = 1 - i_pos_mask\n",
    "    i_element = K.sum(i_pos_mask * x, axis=0)\n",
    "    \n",
    "    j_pos_mask = tf.expand_dims(tf.eye(x_len)[j],axis=-1)\n",
    "    j_neg_mask = 1 - j_pos_mask\n",
    "    j_element = K.sum(j_pos_mask * x, axis=0)\n",
    "    \n",
    "    i_interp_element = t * i_element + (1 - t) * j_element\n",
    "    j_interp_element = t * j_element + (1 - t) * i_element\n",
    "    \n",
    "    x = x * i_neg_mask + i_interp_element * i_pos_mask\n",
    "    x = x * j_neg_mask + j_interp_element * j_pos_mask\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 1. 0. 1.]\n",
      " [1. 0. 1. 0.]\n",
      " [1. 1. 1. 0.]], shape=(5, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(5, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  1.]\n",
      " [ 0. -1.  0. -1.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]], shape=(5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([\n",
    "    [1, 1, 0, 0],\n",
    "    [1, 1, 0, 1],\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 0, 1, 0],\n",
    "    [1, 1, 1, 0]\n",
    "],dtype=tf.float32)\n",
    "t = tf.Variable(0 * tf.ones(tf.shape(x)),dtype=tf.float32)\n",
    "i = tf.Variable(1,dtype=tf.int32)\n",
    "j = tf.Variable(2,dtype=tf.int32)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "#     z = swap(x, i, j)\n",
    "    z = swap(x, i, j, t)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z,x))\n",
    "print(tape.gradient(z,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2.]\n",
      " [1.]\n",
      " [3.]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.]\n",
      " [ 1.]\n",
      " [ 0.]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1],[2],[3]],dtype=tf.float32)\n",
    "t = tf.Variable(0 * tf.ones(tf.shape(x)),dtype=tf.float32)\n",
    "i = tf.Variable(0,dtype=tf.int32)\n",
    "j = tf.Variable(1,dtype=tf.int32)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "#     z = swap(x, i, j)\n",
    "    z = swap(x, i, j, t)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z,x))\n",
    "print(tape.gradient(z,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bubble sort\n",
    "\n",
    "Standard bubble sort implementation with injectable comparator function. It is to be noted that the $t$ parameter is used to decide whether to swap or not instead of having explicit conditionals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def bubble_sort(x, cmp_fun):\n",
    "    '''\n",
    "        Bubble sort\n",
    "        x: Tensor - Expected dims: [array_length, feature_size]\n",
    "        cmp_fun: Function\n",
    "    '''\n",
    "    x_len = tf.shape(x)[0]\n",
    "    for i in range(x_len):\n",
    "        for j in range(i+1, x_len):\n",
    "            cmp_x = tf.concat([x[i], x[j]], axis=0)\n",
    "            cmp_x = tf.reshape(cmp_x, [1, 2, -1])\n",
    "            t = cmp_fun(cmp_x)[0]\n",
    "            x = swap(x, i, j, t)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample comparator function\n",
    "\n",
    "A sample comparator function for testing. The `tf.sign` makes it non-differentiable.\n",
    "\n",
    "For the sake of the example. It counts the number of $1$s in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1.], shape=(1,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def sample_comparator(x):\n",
    "    '''\n",
    "        x: Tensor - Expected dims: [batch_size, 2, feature_size]\n",
    "    '''\n",
    "    sv = tf.reduce_sum(x, axis=-1)\n",
    "    sv = tf.subtract(sv[:,0], sv[:,1])\n",
    "    return 1 - (tf.sign(sv) + 1) / 2\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    x = tf.Variable([\n",
    "        [1,0,0,0],\n",
    "        [1,1,1,1]\n",
    "    ], dtype=tf.float32)\n",
    "    cmp_x = tf.concat([x[0], x[1]], axis=0)\n",
    "    cmp_x = tf.reshape(cmp_x, [1, 2, -1])\n",
    "    cmp_result = sample_comparator(cmp_x)\n",
    "    print(cmp_result)\n",
    "    grad = tape.gradient(cmp_result, x)\n",
    "    print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.]\n",
      " [2.]\n",
      " [3.]], shape=(3, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]], shape=(3, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[3],[1],[2]],dtype=tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = bubble_sort(x, sample_comparator)\n",
    "    print(z)\n",
    "    print(tape.gradient(z,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 0. 0.]\n",
      " [1. 1. 0.]\n",
      " [1. 1. 1.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([\n",
    "    [1, 1, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 1]\n",
    "],dtype=tf.float32)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = bubble_sort(x, sample_comparator)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function swap at 0x000001CBE114F5E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "tf.Tensor(\n",
      "[[1.  0.  0.  0. ]\n",
      " [1.  0.5 0.5 0. ]\n",
      " [1.  0.5 0.5 0. ]\n",
      " [1.  1.  0.5 0.5]\n",
      " [1.  1.  0.5 0.5]], shape=(5, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(5, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([\n",
    "    [1, 1, 0, 0],\n",
    "    [1, 1, 0, 1],\n",
    "    [1, 0, 0, 0],\n",
    "    [1, 0, 1, 0],\n",
    "    [1, 1, 1, 0]\n",
    "],dtype=tf.float32)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = bubble_sort(x, sample_comparator)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen = lambda: np.tril(np.ones((10,10),dtype=np.float32))\n",
    "actual_data = data_gen()\n",
    "actual_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_data = data_gen()\n",
    "np.random.shuffle(shuffled_data)\n",
    "shuffled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function sample_comparator at 0x000001CBDBA4FDC8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function swap at 0x000001CBE114F5E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 10), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = bubble_sort(shuffled_data, sample_comparator)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnable Comparator Function\n",
    "\n",
    "Since the setup is end-to-end differentiable. We can use a DNN as the comparator function and expect it to learn using backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparatorBlock(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ComparatorBlock, self).__init__()\n",
    "        self.dense1 = layers.Dense(10, kernel_initializer=\"he_normal\",activation='relu')\n",
    "        self.dense2 = layers.Dense(10, kernel_initializer=\"he_normal\",activation='relu')\n",
    "        self.dense3 = layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ComparatorBlock, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        vector_len = tf.shape(x)[-1]\n",
    "        h = tf.reshape(x, [-1, 2 * vector_len])\n",
    "        h = self.dense1(h)\n",
    "        h = self.dense2(h)\n",
    "        h = self.dense3(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_comparator = ComparatorBlock()\n",
    "# batch_size = 10\n",
    "# vector_length = 10\n",
    "# input_shape = (batch_size, 2, vector_length)\n",
    "# output_shape = (batch_size, 1)\n",
    "# x = tf.random.normal(input_shape)\n",
    "# y = tf.math.round(tf.random.uniform(output_shape, minval=0, maxval=1))\n",
    "# result = temp_comparator(x)\n",
    "# print(x.shape, result.shape, y.shape)\n",
    "# # print(len(temp_comparator.trainable_variables))\n",
    "\n",
    "# a = Input(shape=(2, vector_length))\n",
    "# b = temp_comparator(a)\n",
    "# m = Model(inputs=a, outputs=b)\n",
    "# m.compile(loss='mse', optimizer='adam')\n",
    "# m.fit(x=x,y=y,epochs=100,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 7 calls to <function swap at 0x000001CBE114F5E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function bubble_sort at 0x000001CBDBA4F5E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "tf.Tensor(16.030499, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "learned_comparator = ComparatorBlock()\n",
    "learned_comparator(tf.zeros((1,2,shuffled_data.shape[-1])))\n",
    "z = bubble_sort(shuffled_data, learned_comparator)\n",
    "# print(z)\n",
    "print(tf.nn.l2_loss(z - actual_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function bubble_sort at 0x000001CBDBA4F5E8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "[[[0.783679068 0.932329535 0 ... -0.0278506912 0.24463512 -0.903960705]\n",
      " [0.642294288 0.776055098 0 ... -0.00857573748 0.182930738 -0.723977387]\n",
      " [0.563284457 0.672721684 0 ... -0.0109619275 0.143761888 -0.60566175]\n",
      " ...\n",
      " [0.436410964 0.465857357 0 ... -0.0219736807 0.0640842244 -0.465248823]\n",
      " [0.290339291 0.285001934 0 ... -0.0156117454 0.0527317 -0.297663778]\n",
      " [0.26131022 0.246905908 0 ... -0.0153777292 0.0523141176 -0.281999111]], [0.783679068 0.932329535 0 ... -0.0278506912 0.24463512 -0.903960705], [[4.47132301 -0.224669605 -0.704437256 ... 0 -3.05646396 -2.78740692]\n",
      " [3.67464828 -0.208235726 -0.604034722 ... 0 -2.51188135 -2.2907629]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0.00105429813 -0.00538781192 0.0422475114 ... 0 -0.000720694661 -0.000657245517]\n",
      " [0.0124988221 -0.00527670793 0.0423009917 ... 0 -0.00854383223 -0.00779172592]\n",
      " [0.918580711 -0.0355195403 -0.161287606 ... 0 -0.627914667 -0.572640061]], [2.05973172 -0.107041143 -0.262543887 ... 0 -1.40797186 -1.28402972], [[-6.49071932]\n",
      " [-0.105420031]\n",
      " [0.00129549531]\n",
      " ...\n",
      " [0]\n",
      " [-3.40897417]\n",
      " [-3.20549536]], [-3.33910036]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(shuffled_data, dtype=tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = bubble_sort(x, learned_comparator)\n",
    "    loss = tf.nn.l2_loss(z - actual_data)\n",
    "    grads = tape.gradient(loss, learned_comparator.trainable_variables)\n",
    "    tf.print(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We can train the setup end-to-end withing Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0304985\n",
      "10.5972567\n",
      "5.54998255\n",
      "4.58355618\n",
      "3.0567193\n",
      "1.97006762\n",
      "1.39458871\n",
      "0.917508185\n",
      "0.507980704\n",
      "0.332600266\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(shuffled_data, dtype=tf.float32)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = bubble_sort(x, learned_comparator)\n",
    "        loss = tf.nn.l2_loss(z - actual_data)\n",
    "    var_list = learned_comparator.trainable_variables\n",
    "    grads = tape.gradient(loss, var_list)\n",
    "    opt.apply_gradients(zip(grads, var_list))\n",
    "    return loss\n",
    "\n",
    "for i in range(1000):\n",
    "    loss = train_step()\n",
    "    if i % 100 == 0:\n",
    "        tf.print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]], shape=(10, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "z = bubble_sort(x, learned_comparator)\n",
    "z = tf.round(z)\n",
    "print(z - actual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
