{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable indexing of arrays\n",
    "Since, table or array lookup is an inherently non-differentiable process, the autograd is unable to resolve gradients of the result with respect to the output.\n",
    "\n",
    "Here we have some strategies which gives a valid gradient with respect to both the index and input array. The choice of the strategy is problem specific. Also, this is by no means an exhaustive list. You can help by expanding it.\n",
    "\n",
    "Abbreviation: WRT = With respect to\n",
    "\n",
    "Further Reading: [Neural Turing Machines](https://arxiv.org/abs/1410.5401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive lookup\n",
    "Naive lookup does produce a gradient wrt its input array but not wrt the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "tf.Tensor([0. 0. 1. 0. 0.], shape=(5,), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def naive_lookup(arr, index):\n",
    "    index = tf.round(index)\n",
    "    index = tf.cast(index, tf.int32)\n",
    "    result = arr[index]\n",
    "    return result\n",
    "\n",
    "arr = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "index = tf.Variable(1.5, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = naive_lookup(arr, index)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, arr))\n",
    "print(tape.gradient(z, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superposition lookup\n",
    "In this method, we have a distribution instead of an integer index. This distribution usually comes after a softmax operation. The result is the dot product of the index and the input array. This is a very popular method in DNN literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0000002, shape=(), dtype=float32)\n",
      "tf.Tensor([0.  0.1 0.8 0.  0.1], shape=(5,), dtype=float32)\n",
      "tf.Tensor([1. 2. 3. 5. 4.], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def superposition_lookup(arr, indices):\n",
    "    result = arr * indices\n",
    "    return tf.reduce_sum(result, axis=0)\n",
    "\n",
    "arr = tf.Variable([1,2,3,5,4],dtype=tf.float32)\n",
    "indices = tf.Variable([0.0, 0.1, 0.8, 0.0, 0.1], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "#     indices = tf.nn.softmax(indices)\n",
    "    z = superposition_lookup(arr, indices)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, arr))\n",
    "print(tape.gradient(z, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear lookup\n",
    "In this method we use [linear interpolation](https://en.wikipedia.org/wiki/Linear_interpolation) to interpolate between the two nearest candidates. For 2D arrays, [Bilinear interpolation](https://en.wikipedia.org/wiki/Bilinear_interpolation) can be used.\n",
    "\n",
    "This gives a well defined gradient wrt to both the input and index. However, it is a soft lookup and can return values not present in the array itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.5, shape=(), dtype=float32)\n",
      "tf.Tensor([0.  0.5 0.5 0.  0. ], shape=(5,), dtype=float32)\n",
      "tf.Tensor(-1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def linear_lookup(arr, index):\n",
    "    t1 = tf.math.floor(index)\n",
    "    t2 = tf.math.ceil(index)\n",
    "    \n",
    "    t = tf.math.divide_no_nan((index - t1), (t2 - t1))\n",
    "    \n",
    "    i1 = tf.cast(t1, tf.int32)\n",
    "    i2 = tf.cast(t2, tf.int32)\n",
    "    \n",
    "    # Linear interpolation\n",
    "    result = t * arr[i1] + (1 - t) * arr[i2]\n",
    "    \n",
    "    return result\n",
    "\n",
    "arr = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "index = tf.Variable(1.5, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = linear_lookup(arr, index)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, arr))\n",
    "print(tape.gradient(z, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual lookup\n",
    "In this method, we return two tensors, the result and the residue. So, although the result is not differentiable wrt to index, the residue is. This allows us to propagate some extra information in parallel which can then be consumed intelligently by some algorithm in downstream. This has the benefit that the result always exists in the original array and is never an interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -0.5\n",
      "[0 0 1 0 0] [0 0 0 0 0]\n",
      "0 1\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def residual_lookup(arr, index):\n",
    "    i = tf.round(index)\n",
    "    residue = index - i\n",
    "    i = tf.cast(i, tf.int32)\n",
    "    \n",
    "    result = arr[i]\n",
    "    \n",
    "    return result, residue\n",
    "\n",
    "arr = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "index = tf.Variable(1.5, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    result, residue = residual_lookup(arr, index)\n",
    "\n",
    "tf.print(result, residue)\n",
    "tf.print(tape.gradient(result, arr), tape.gradient(residue, arr))\n",
    "tf.print(tape.gradient(result, index), tape.gradient(residue, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array assignment\n",
    "Tensorflow does not support direct index assignment of variables. So, instead we use a masking technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [4. 4. 4.]\n",
      " [3. 3. 3.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]], shape=(3, 3), dtype=float32)\n",
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "None\n",
      "tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def assign_index(arr, index, element):\n",
    "    arr_shape = tf.shape(arr)\n",
    "    \n",
    "    pos_mask = tf.eye(arr_shape[0])[index]\n",
    "    pos_mask = tf.transpose(tf.expand_dims(pos_mask, 0))\n",
    "    neg_mask = 1 - pos_mask\n",
    "    \n",
    "    tiled_element = tf.reshape(tf.tile(element, [arr_shape[0]]), arr_shape)\n",
    "    \n",
    "    arr = arr * neg_mask + tiled_element * pos_mask\n",
    "    \n",
    "    return arr\n",
    "\n",
    "arr = tf.Variable([\n",
    "    [1,1,1],\n",
    "    [2,2,2],\n",
    "    [3,3,3]\n",
    "],dtype=tf.float32)\n",
    "index = tf.constant(1)\n",
    "element = tf.Variable([4,4,4],dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    new_arr = assign_index(arr, index, element)\n",
    "    \n",
    "print(new_arr)\n",
    "print(tape.gradient(new_arr, arr))\n",
    "print(tape.gradient(new_arr, index))\n",
    "print(tape.gradient(new_arr, element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
