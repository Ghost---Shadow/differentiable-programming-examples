{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable indexing of arrays\n",
    "Since, table or array lookup is an inherently non-differentiable process, the autograd is unable to resolve gradients of the result with respect to the output.\n",
    "\n",
    "Here we have some strategies which gives a valid gradient with respect to both the index and input array. The choice of the strategy is problem specific. Also, this is by no means an exhaustive list. You can help by expanding it.\n",
    "\n",
    "Abbreviation: WRT = With respect to\n",
    "\n",
    "Further Reading: [Neural Turing Machines](https://arxiv.org/abs/1410.5401)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive lookup\n",
    "Naive lookup does produce a gradient wrt its input array but not wrt the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3.0, shape=(), dtype=float32)\n",
      "tf.Tensor([0. 0. 1. 0. 0.], shape=(5,), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def naive_lookup(arr, index):\n",
    "    index = tf.round(index)\n",
    "    index = tf.cast(index, tf.int32)\n",
    "    result = arr[index]\n",
    "    return result\n",
    "\n",
    "arr = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "index = tf.Variable(1.5, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = naive_lookup(arr, index)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, arr))\n",
    "print(tape.gradient(z, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear lookup\n",
    "In this method we use [linear interpolation](https://en.wikipedia.org/wiki/Linear_interpolation) to interpolate between the two nearest candidates. For 2D arrays, [Bilinear interpolation](https://en.wikipedia.org/wiki/Bilinear_interpolation) can be used.\n",
    "\n",
    "This gives a well defined gradient wrt to both the input and index. However, it is a soft lookup and can return values not present in the array itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def interp_factor(index):\n",
    "    t1 = tf.math.floor(index)\n",
    "    t2 = tf.math.ceil(index)\n",
    "    \n",
    "    t = tf.math.divide_no_nan((index - t1), (t2 - t1))\n",
    "    \n",
    "    i1 = tf.cast(t1, tf.int32)\n",
    "    i2 = tf.cast(t2, tf.int32)\n",
    "    \n",
    "    return t, i1, i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.5, shape=(), dtype=float32)\n",
      "tf.Tensor([0.  0.5 0.5 0.  0. ], shape=(5,), dtype=float32)\n",
      "tf.Tensor(-1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def linear_lookup(arr, index):\n",
    "    t, i1, i2 = interp_factor(index)\n",
    "    \n",
    "    # Linear interpolation\n",
    "    result = t * arr[i1] + (1 - t) * arr[i2]\n",
    "    \n",
    "    return result\n",
    "\n",
    "arr = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "index = tf.Variable(1.5, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = linear_lookup(arr, index)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, arr))\n",
    "print(tape.gradient(z, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Superposition lookup\n",
    "In this method, we have a distribution instead of an integer index. This distribution usually comes after a softmax operation. The result is the dot product of the index and the input array. This is a very popular method in DNN literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.  0.  0.5 0.  0.5], shape=(5,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.5 0.5 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 0.5 0.5]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  0. ]], shape=(5, 5), dtype=float32)\n",
      "tf.Tensor([1. 1. 1. 1. 1.], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def superposition_lookup_vectored(arr, indices):\n",
    "    if tf.rank(arr) == 1:\n",
    "        arr = tf.expand_dims(arr, -1)\n",
    "    indices = tf.expand_dims(indices, -1)\n",
    "    result = arr * indices\n",
    "    return tf.reduce_sum(result, axis=0)\n",
    "\n",
    "arr = tf.Variable([\n",
    "    [0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1],\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0],\n",
    "],dtype=tf.float32)\n",
    "indices = tf.Variable([0.5, 0.5, 0, 0, 0], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "#     indices = tf.nn.softmax(indices)\n",
    "    z = superposition_lookup_vectored(arr, indices)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, arr))\n",
    "print(tape.gradient(z, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([3.0000002], shape=(1,), dtype=float32)\n",
      "tf.Tensor([0.  0.1 0.8 0.  0.1], shape=(5,), dtype=float32)\n",
      "tf.Tensor([1. 2. 3. 5. 4.], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "arr = tf.Variable([1,2,3,5,4],dtype=tf.float32)\n",
    "indices = tf.Variable([0.0, 0.1, 0.8, 0.0, 0.1], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "#     indices = tf.nn.softmax(indices)\n",
    "    z = superposition_lookup_vectored(arr, indices)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, arr))\n",
    "print(tape.gradient(z, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.  0.  0.5 0.5 0. ], shape=(5,), dtype=float32)\n",
      "tf.Tensor([2.5], shape=(1,), dtype=float32)\n",
      "tf.Tensor(-1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def bandwidthify(index, bandwidth):\n",
    "    t, i1, i2 = interp_factor(index)\n",
    "    \n",
    "    # Prevent array out of bounds\n",
    "    i1 = tf.clip_by_value(i1, 0, bandwidth - 1)\n",
    "    i2 = tf.clip_by_value(i2, 0, bandwidth - 1)\n",
    "    t = tf.clip_by_value(t, 0, 1)\n",
    "    \n",
    "    # Linear interpolation\n",
    "    eye = tf.eye(bandwidth)\n",
    "    result = t * eye[i1] + (1 - t) * eye[i2]\n",
    "    \n",
    "    return result\n",
    "\n",
    "index = tf.Variable(2.5, dtype=tf.float32)\n",
    "bandwidth = tf.constant(5, dtype=tf.int32)\n",
    "dummy_array = tf.cast(tf.range(bandwidth), tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = bandwidthify(index, bandwidth)\n",
    "    nz = superposition_lookup_vectored(dummy_array, z) # Lookup operation\n",
    "\n",
    "print(z)\n",
    "print(nz)\n",
    "print(tape.gradient(nz, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2.5], shape=(1,), dtype=float32)\n",
      "tf.Tensor([0.  0.5 0.5 0.  0. ], shape=(5,), dtype=float32)\n",
      "tf.Tensor(-1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def superposition_lookup(arr, index):\n",
    "    bandwidth = tf.shape(arr)[0]\n",
    "    vectored_index = bandwidthify(index, bandwidth)\n",
    "    result = superposition_lookup_vectored(arr, vectored_index)\n",
    "    \n",
    "    return result\n",
    "\n",
    "arr = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "index = tf.Variable(1.5, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = superposition_lookup(arr, index)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, arr))\n",
    "print(tape.gradient(z, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function superposition_lookup_vectored at 0x0000024B8BF2A8B8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "tf.Tensor(\n",
      "[[0.  1.  0.  0.  0. ]\n",
      " [0.  0.  1.  0.  0. ]\n",
      " [0.  0.  0.  0.5 0.5]\n",
      " [1.  0.  0.  0.  0. ]\n",
      " [0.  0.  0.  0.  1. ]], shape=(5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. ]\n",
      " [1. ]\n",
      " [2. ]\n",
      " [1.5]\n",
      " [6. ]], shape=(5, 1), dtype=float32)\n",
      "tf.Tensor([ 0.  0. -1.  0.  0.], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def bulk_bandwidthify(indices, bandwidth):\n",
    "    num_indices = tf.shape(indices)[0]\n",
    "    \n",
    "    indices = tf.unstack(indices)\n",
    "    result = tf.zeros((num_indices, bandwidth), dtype=tf.float32)\n",
    "    result = tf.unstack(result)\n",
    "    \n",
    "    for i, index in enumerate(indices):\n",
    "        b_index = bandwidthify(index, bandwidth)\n",
    "        result[i] += b_index\n",
    "    \n",
    "    result = tf.stack(result)\n",
    "    return result\n",
    "\n",
    "indices = tf.Variable([1,2,3.5,0,4],dtype=tf.float32)\n",
    "bandwidth = tf.constant(5, dtype=tf.int32)\n",
    "dummy_array = tf.cast(tf.range(bandwidth), tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = bulk_bandwidthify(indices, bandwidth)\n",
    "    nz = superposition_lookup_vectored(dummy_array, z) # Lookup operation\n",
    "\n",
    "print(z)\n",
    "print(nz)\n",
    "print(tape.gradient(nz, indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual lookup\n",
    "In this method, we return two tensors, the result and the residue. So, although the result is not differentiable wrt to index, the residue is. This allows us to propagate some extra information in parallel which can then be consumed intelligently by some algorithm in downstream. This has the benefit that the result always exists in the original array and is never an interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 -0.5\n",
      "[0 0 1 0 0] [0 0 0 0 0]\n",
      "0 1\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def residual_lookup(arr, index):\n",
    "    i = tf.round(index)\n",
    "    residue = index - i\n",
    "    i = tf.cast(i, tf.int32)\n",
    "    \n",
    "    result = arr[i]\n",
    "    \n",
    "    return result, residue\n",
    "\n",
    "arr = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "index = tf.Variable(1.5, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    result, residue = residual_lookup(arr, index)\n",
    "\n",
    "tf.print(result, residue)\n",
    "tf.print(tape.gradient(result, arr), tape.gradient(residue, arr))\n",
    "tf.print(tape.gradient(result, index), tape.gradient(residue, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array assignment\n",
    "Tensorflow does not support direct index assignment of variables. So, instead we use a masking technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [4. 4. 4.]\n",
      " [3. 3. 3.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [0. 0. 0.]\n",
      " [1. 1. 1.]], shape=(3, 3), dtype=float32)\n",
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "None\n",
      "tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def assign_index(arr, index, element):\n",
    "    arr_shape = tf.shape(arr)\n",
    "    \n",
    "    pos_mask = tf.eye(arr_shape[0])[index]\n",
    "    pos_mask = tf.transpose(tf.expand_dims(pos_mask, 0))\n",
    "    neg_mask = 1 - pos_mask\n",
    "    \n",
    "    tiled_element = tf.reshape(tf.tile(element, [arr_shape[0]]), arr_shape)\n",
    "    \n",
    "    arr = arr * neg_mask + tiled_element * pos_mask\n",
    "    \n",
    "    return arr\n",
    "\n",
    "arr = tf.Variable([\n",
    "    [1,1,1],\n",
    "    [2,2,2],\n",
    "    [3,3,3]\n",
    "],dtype=tf.float32)\n",
    "index = tf.constant(1)\n",
    "element = tf.Variable([4,4,4],dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    new_arr = assign_index(arr, index, element)\n",
    "    \n",
    "print(new_arr)\n",
    "print(tape.gradient(new_arr, arr))\n",
    "print(tape.gradient(new_arr, index))\n",
    "print(tape.gradient(new_arr, element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
