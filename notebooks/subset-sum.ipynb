{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Sum Problem\n",
    "The [subset sum problem](https://en.wikipedia.org/wiki/Subset_sum_problem) is defined as, given a set of numbers, find a subset which adds up to another number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "For example let us have a set $S$ and a target $T$\n",
    "\n",
    "$$\n",
    "T = 7.0\n",
    "$$\n",
    "\n",
    "\\begin{equation*}\n",
    "S = \n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0 & 3.0 & 4.0 & 5.0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Our goal is to find a mask $M$, such that, the dot product results in the target. Here is an example of a mask that adds up to our target.\n",
    "\n",
    "\\begin{equation*}\n",
    "M = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 & 1.0 & 1.0 & 0.0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "We can verify that \n",
    "\n",
    "$$ T = M \\cdot S $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7.0, shape=(), dtype=float32)\n",
      "tf.Tensor([0. 0. 1. 1. 0.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([1. 2. 3. 4. 5.], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def compute_subset_sum(S, M):\n",
    "    return tf.tensordot(S, M, 1)\n",
    "\n",
    "S = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "M = tf.Variable([0,0,1,1,0],dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    T_ = compute_subset_sum(S, M)\n",
    "    \n",
    "print(T_)\n",
    "print(tape.gradient(T_, S))\n",
    "print(tape.gradient(T_, M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "However, if we train as is, we find that $M$ is not a mask but it forms a linear combination with its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(5e-3)\n",
    "\n",
    "@tf.function\n",
    "def train_step(S, M, T):\n",
    "    with tf.GradientTape() as tape:\n",
    "        T_ = compute_subset_sum(S, M)\n",
    "        loss = tf.nn.l2_loss(T_ - T)\n",
    "    \n",
    "    grads = tape.gradient(loss, M)\n",
    "    opt.apply_gradients(zip([grads], [M]))\n",
    "    \n",
    "    return loss, T_\n",
    "\n",
    "S = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "M = tf.Variable([1,1,1,1,1],dtype=tf.float32)\n",
    "T = 7\n",
    "\n",
    "for i in range(1000):\n",
    "    loss, T_ = train_step(S, M, T)\n",
    "    if i % 100 == 0:\n",
    "        actual = compute_subset_sum(S, tf.round(M))\n",
    "        tf.print(loss, M, T_, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bistable loss\n",
    "To force the values to be close to 0 and 1, we introduce the [Bistable Loss](notebooks/boolean-satisfiability.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.loss import bistable_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retraining we find that each element the mask is now closer to 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(5e-3)\n",
    "\n",
    "@tf.function\n",
    "def train_step(S, M, T):\n",
    "    with tf.GradientTape() as tape:\n",
    "        T_ = compute_subset_sum(S, M)\n",
    "        loss = tf.nn.l2_loss(T_ - T)\n",
    "        loss += tf.reduce_sum(bistable_loss(M)) * 10\n",
    "    \n",
    "    grads = tape.gradient(loss, M)\n",
    "    opt.apply_gradients(zip([grads], [M]))\n",
    "    \n",
    "    return loss, T_\n",
    "\n",
    "S = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "M = tf.Variable([1,1,1,1,1],dtype=tf.float32)\n",
    "T = 7\n",
    "\n",
    "for i in range(1000):\n",
    "    loss, T_ = train_step(S, M, T)\n",
    "    if i % 100 == 0:\n",
    "        actual = compute_subset_sum(S, tf.round(M))\n",
    "        tf.print(loss, M, T_, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot softmax\n",
    "\n",
    "To further make sure that the mask remains either 0 or 1, we increase the dimentionality of the $M$ and apply softmax along the vertical axis.\n",
    "\n",
    "\\begin{equation*}\n",
    "M = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 1 & 1 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "becomes\n",
    "\n",
    "\\begin{equation*}\n",
    "M = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 1 & 1 & 0 \\\\\n",
    "1 & 1 & 0 & 0 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Therefore, $\\bar{T}$ becomes\n",
    "\n",
    "$$ \\bar{T} = softmax(M, axis=1) \\cdot S $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_subset_sum_v2(S, M):\n",
    "    M = tf.transpose(M)\n",
    "    return tf.tensordot(S, M[1], 1)\n",
    "\n",
    "S = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "M = tf.Variable(tf.one_hot([0,0,1,1,0], 2),dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    M_s = tf.nn.softmax(M, axis=1)\n",
    "    T_ = compute_subset_sum_v2(S, M_s)\n",
    "\n",
    "tf.print(tf.transpose(M))\n",
    "tf.print(T_)\n",
    "tf.print(tape.gradient(T_, S))\n",
    "tf.print(tape.gradient(T_, M))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(5e-3)\n",
    "\n",
    "@tf.function\n",
    "def train_step(S, M, T):\n",
    "    with tf.GradientTape() as tape:\n",
    "        M_s = tf.nn.softmax(M, axis=1)\n",
    "        T_ = compute_subset_sum_v2(S, M_s)\n",
    "        loss = tf.nn.l2_loss(T_ - T)\n",
    "        loss += tf.reduce_sum(bistable_loss(M_s)) * 10\n",
    "    \n",
    "    grads = tape.gradient(loss, M)\n",
    "    opt.apply_gradients(zip([grads], [M]))\n",
    "    \n",
    "    return loss, T_\n",
    "\n",
    "S = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "M = tf.Variable(tf.one_hot([1,1,1,1,1], 2), dtype=tf.float32)\n",
    "T = 7\n",
    "\n",
    "for i in range(1000):\n",
    "    loss, T_ = train_step(S, M, T)\n",
    "    if i % 100 == 0:\n",
    "        M_T = tf.transpose(M)\n",
    "        M_T = tf.nn.softmax(M_T, axis=0)\n",
    "        actual = compute_subset_sum_v2(S, tf.round(M))\n",
    "        tf.print(loss, M_T[1], T_, actual)\n",
    "        \n",
    "tf.print(M_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
