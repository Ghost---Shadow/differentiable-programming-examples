{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0 0 0]\n",
      "0 [1 0 0]\n",
      "0.707106769 [0.707106829 -0.707106709 0]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def is_phi(element):\n",
    "    tf.debugging.assert_rank(element, 1)\n",
    "    \n",
    "    elem_dim = tf.shape(element)[0]\n",
    "    phi = tf.one_hot(0, elem_dim)\n",
    "    \n",
    "    element = tf.math.l2_normalize(element)\n",
    "    t = tf.tensordot(element, phi, axes=1)\n",
    "\n",
    "    return t\n",
    "\n",
    "test1 = tf.Variable([1,0,0], dtype=tf.float32)\n",
    "test2 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "test3 = tf.Variable([.5,.5,0], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    result1 = is_phi(test1)\n",
    "    result2 = is_phi(test2)\n",
    "    result3 = is_phi(test3)\n",
    "\n",
    "tf.print(result1, tape.gradient(result1, test1))\n",
    "tf.print(result2, tape.gradient(result2, test2))\n",
    "tf.print(result3, tape.gradient(result3, test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.stacks import stack_push, stack_pop, stack_peek, new_stack, new_stack_from_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0.0428932235 0.0428932235 0.707106769]\n",
      " [0 0.707106769 0.0857864469]]\n",
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "[0.707106769 0.292893231 0]\n",
      "[1 0 0]\n",
      "[0.0606601834 0.792893231 0.792893231]\n",
      "([[0 0 0]\n",
      " [0.207106784 0.207106784 0.207106784]\n",
      " [0.207106799 0.207106799 0.207106799]], [2.87867975 1.53553391 1.76776707])\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def safe_push(stack, element, is_phi_fn):\n",
    "    tf.debugging.assert_rank_at_least(stack[0], 2)\n",
    "    tf.debugging.assert_rank(stack[1], 1)\n",
    "    tf.debugging.assert_equal(tf.shape(stack[0])[1:], tf.shape(element))\n",
    "    tf.debugging.assert_equal(tf.rank(stack[0]) - 1, tf.rank(element) )\n",
    "    \n",
    "    t = is_phi_fn(element)\n",
    "    \n",
    "    old_buffer, old_index = stack\n",
    "    new_buffer, new_index = stack_push(stack, element)\n",
    "\n",
    "    buffer = t * old_buffer + (1 - t) * new_buffer\n",
    "    index = t * old_index + (1 - t) * new_index\n",
    "\n",
    "    # Hack to tell tensorflow that the shape has not changed\n",
    "    # TODO: Why does this hack work?\n",
    "    buffer = tf.reshape(buffer, tf.shape(old_buffer))\n",
    "    index = tf.reshape(index, tf.shape(old_index))\n",
    "\n",
    "    new_stack = (buffer, index)\n",
    "\n",
    "    return new_stack\n",
    "\n",
    "stack = new_stack((3,3), True)\n",
    "original_stack = stack\n",
    "\n",
    "element1 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "element2 = tf.Variable([0.5,0.5,0], dtype=tf.float32)\n",
    "element3 = tf.Variable([0,0,1], dtype=tf.float32)\n",
    "element4 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    stack = safe_push(stack, element1, is_phi)\n",
    "    stack = safe_push(stack, element2, is_phi)\n",
    "    stack = safe_push(stack, element3, is_phi)\n",
    "    stack = safe_push(stack, element4, is_phi)\n",
    "    \n",
    "tf.print(stack[0])\n",
    "tf.print(tf.round(stack[0]))\n",
    "tf.print(stack[1])\n",
    "tf.print(tf.round(stack[1]))\n",
    "tf.print(tape.gradient(stack[0], element3))\n",
    "tf.print(tape.gradient(stack, original_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[1 1 1]\n",
      " [1 1 1]\n",
      " [1 0 0]], [0 0 1])\n",
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def pop_and_purge(stack):\n",
    "    stack_len = tf.shape(stack[0])[1]\n",
    "    phi = tf.one_hot(0, stack_len, dtype=tf.float32)\n",
    "    stack, element = stack_pop(stack)\n",
    "    stack = stack_push(stack, phi)\n",
    "    stack, _ = stack_pop(stack)\n",
    "    \n",
    "    return stack, element\n",
    "\n",
    "stack = new_stack_from_buffer(tf.ones((3,3), dtype=tf.float32))\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    stack, element = pop_and_purge(stack)\n",
    "    \n",
    "tf.print(stack)\n",
    "tf.print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "[1 0 0]\n",
      "[-1 1 1]\n",
      "([[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]], [4 1 1])\n"
     ]
    }
   ],
   "source": [
    "stack = new_stack((3,3), True)\n",
    "\n",
    "element1 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "element2 = tf.Variable([1,0,0], dtype=tf.float32)\n",
    "element3 = tf.Variable([0,0,1], dtype=tf.float32)\n",
    "element4 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "\n",
    "original_stack = stack\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    stack = safe_push(stack, element1, is_phi)\n",
    "    stack = safe_push(stack, element2, is_phi)\n",
    "    stack = safe_push(stack, element3, is_phi)\n",
    "    stack = safe_push(stack, element4, is_phi)\n",
    "    \n",
    "tf.print(stack[0])\n",
    "tf.print(stack[1])\n",
    "tf.print(tape.gradient(stack[0], element3))\n",
    "tf.print(tape.gradient(stack, original_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.array_ops import tensor_lookup_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_DIM = 6\n",
    "PRODUCTION_DIM = 4\n",
    "STACK_SIZE = 10\n",
    "PHI = np.eye(TOKEN_DIM)[0]\n",
    "S = np.eye(TOKEN_DIM)[1]\n",
    "O = np.eye(TOKEN_DIM)[2]\n",
    "T = np.eye(TOKEN_DIM)[3]\n",
    "X = np.eye(TOKEN_DIM)[4]\n",
    "PLUS = np.eye(TOKEN_DIM)[5]\n",
    "\n",
    "E = [PHI, PHI, PHI]\n",
    "\n",
    "G_s = tf.constant([\n",
    "    [E, E, E, E, E, E],\n",
    "    [E, E, E, E, E, E],\n",
    "    [E, [S, O, T], E, E, E, E],\n",
    "    [E, [T, PHI, PHI], E, E, E, E],\n",
    "], dtype=tf.float32)\n",
    "G_o = tf.constant([\n",
    "    [E, E, E, [X, PHI, PHI], E, E],\n",
    "    [E, E, [PLUS, PHI, PHI], E, E, E],\n",
    "    [E, E, E, E, E, E],\n",
    "    [E, E, E, E, E, E],\n",
    "], dtype=tf.float32)\n",
    "grammar = (G_s, G_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_ S O T x + '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_pretty_print(tokens):\n",
    "    tokens = tf.argmax(tokens, axis=1)\n",
    "    lookup = ['_', 'S', 'O', 'T', 'x', '+']\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    for token in tokens:\n",
    "        result += f'{lookup[token]} '\n",
    "        \n",
    "    return result\n",
    "\n",
    "tokens = tf.transpose(tf.one_hot([0,1,2,3,4,5], TOKEN_DIM, dtype=tf.float32))\n",
    "tokens_pretty_print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\windows\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_ops.py:4051: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function assign_index_vectored at 0x0000029B7EC70288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function stack_push at 0x0000029B7EC72C18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "T O S _ _ _ _ _ _ _ \n",
      "[-0.0833333358 -0.0833333358 3 -0.0416666679]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def production_step(grammar, production, stack, output, is_phi_fn):\n",
    "    tf.debugging.assert_rank(grammar[0], 4)\n",
    "    tf.debugging.assert_rank(grammar[1], 4)\n",
    "    tf.debugging.assert_rank(production, 1)\n",
    "    tf.debugging.assert_rank(stack[0], 2)\n",
    "    tf.debugging.assert_rank(output[0], 2)\n",
    "    \n",
    "    G_s, G_o = grammar\n",
    "    \n",
    "    # Save the shapes\n",
    "    stack_0_shape = tf.shape(stack[0])\n",
    "    stack_1_shape = tf.shape(stack[1])\n",
    "    output_0_shape = tf.shape(output[0])\n",
    "    output_1_shape = tf.shape(output[1])\n",
    "    \n",
    "    # Get next token from stack\n",
    "    stack, stack_top_token = pop_and_purge(stack)\n",
    "\n",
    "    # Push tokens back onto the stack\n",
    "    tokens_to_push = tensor_lookup_2d(G_s, production, stack_top_token)\n",
    "    for token in tf.reverse(tokens_to_push, axis=[0]):\n",
    "        stack = safe_push(stack, token, is_phi_fn)\n",
    "    \n",
    "    # Push tokens to output\n",
    "    tokens_to_push = tensor_lookup_2d(G_o, production, stack_top_token)\n",
    "    for token in tokens_to_push:\n",
    "        output = safe_push(output, token, is_phi_fn)\n",
    "    \n",
    "    return stack, output\n",
    "\n",
    "stack = new_stack(((STACK_SIZE, TOKEN_DIM)))\n",
    "output = new_stack(((STACK_SIZE, TOKEN_DIM)))\n",
    "\n",
    "stack = safe_push(stack, tf.constant(S, dtype=tf.float32), is_phi)\n",
    "production = tf.one_hot(2, PRODUCTION_DIM)\n",
    "\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    tape.watch(grammar)\n",
    "    tape.watch(production)\n",
    "    tape.watch(stack)\n",
    "    tape.watch(output)\n",
    "    \n",
    "    new_s, new_o = production_step(grammar, production, stack, output, is_phi)\n",
    "\n",
    "tf.print(tokens_pretty_print(new_s[0]))\n",
    "# tf.print(tape.gradient(new_o, output))\n",
    "# tf.print(tape.gradient(new_s, stack))\n",
    "# tf.print(tape.gradient(new_s[0], grammar[0]).shape)\n",
    "# tf.print(tape.gradient(new_s[1], grammar[0]).shape)\n",
    "# tf.print(tape.gradient(new_o[0], grammar[1]).shape)\n",
    "# tf.print(tape.gradient(new_o[1], grammar[1]).shape)\n",
    "tf.print(tape.gradient(new_s, production))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p\t 2\n",
      "G_s\t S O T  (2, 1)\n",
      "G_o\t _ _ _  (2, 1)\n",
      "S_i\t S _ _ _ _ _ _ _ _ _  1\n",
      "S_i+1\t T O S _ _ _ _ _ _ _  3\n",
      "O_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "p\t 3\n",
      "G_s\t T _ _  (3, 1)\n",
      "G_o\t _ _ _  (3, 1)\n",
      "S_i\t T O S _ _ _ _ _ _ _  3\n",
      "S_i+1\t T O T _ _ _ _ _ _ _  3\n",
      "O_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "p\t 0\n",
      "G_s\t _ _ _  (0, 3)\n",
      "G_o\t x _ _  (0, 3)\n",
      "S_i\t T O T _ _ _ _ _ _ _  3\n",
      "S_i+1\t T O _ _ _ _ _ _ _ _  2\n",
      "O_i+1\t x _ _ _ _ _ _ _ _ _  1\n",
      "p\t 1\n",
      "G_s\t _ _ _  (1, 2)\n",
      "G_o\t + _ _  (1, 2)\n",
      "S_i\t T O _ _ _ _ _ _ _ _  2\n",
      "S_i+1\t T _ _ _ _ _ _ _ _ _  1\n",
      "O_i+1\t x + _ _ _ _ _ _ _ _  2\n",
      "p\t 0\n",
      "G_s\t _ _ _  (0, 3)\n",
      "G_o\t x _ _  (0, 3)\n",
      "S_i\t T _ _ _ _ _ _ _ _ _  1\n",
      "S_i+1\t _ _ _ _ _ _ _ _ _ _  0\n",
      "O_i+1\t x + x _ _ _ _ _ _ _  3\n"
     ]
    }
   ],
   "source": [
    "# @tf.function\n",
    "def generate(grammar, productions, stack_shape, S, is_phi_fn):\n",
    "    # Reserve space for stack and output\n",
    "    stack = new_stack(stack_shape)\n",
    "    output = new_stack(stack_shape)\n",
    "    \n",
    "    # Push S to top of stack\n",
    "    stack = safe_push(stack, S, is_phi)\n",
    "\n",
    "    for production in productions:\n",
    "        top = stack_peek(stack)\n",
    "        \n",
    "        before = tokens_pretty_print(stack[0]), tf.argmax(stack[1])\n",
    "        tf.print('p\\t', tf.argmax(production))\n",
    "        i = tf.argmax(production)\n",
    "        j = tf.argmax(top)\n",
    "        tf.print('G_s\\t', tokens_pretty_print(G_s[i][j]), (i,j))\n",
    "        tf.print('G_o\\t', tokens_pretty_print(G_o[i][j]), (i,j))\n",
    "        \n",
    "        stack, output = production_step(grammar, production, stack, output, is_phi_fn)\n",
    "        \n",
    "        tf.print('S_i\\t', before[0], before[1])\n",
    "        tf.print('S_i+1\\t', tokens_pretty_print(stack[0]), tf.argmax(stack[1]))\n",
    "        tf.print('O_i+1\\t', tokens_pretty_print(output[0]), tf.argmax(output[1]))\n",
    "        \n",
    "        \n",
    "    return output\n",
    "\n",
    "productions = tf.one_hot([2, 3, 0, 1, 0], PRODUCTION_DIM)\n",
    "\n",
    "stack_shape = (STACK_SIZE, TOKEN_DIM)\n",
    "d_S = tf.constant(S, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    output = generate(grammar, productions, stack_shape, d_S, is_phi)\n",
    "    \n",
    "# tf.print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
