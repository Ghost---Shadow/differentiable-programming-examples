{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacks\n",
    "\n",
    "There exist many implementations of differentiable stacks in the literature related to neural turing machines and similar. e.g. [Learning to Transduce with Unbounded Memory](http://papers.nips.cc/paper/5648-learning-to-transduce-with-unbounded-memory.pdf), [Inferring Algorithmic Patterns withStack-Augmented Recurrent Nets](https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.pdf) e.t.c.\n",
    "\n",
    "However, here we restrict ourselves to follow these two rules. \n",
    "1. It must be deterministic and lossless in forward pass\n",
    "2. It must have well definied gradients in backward pass\n",
    "\n",
    "Tensorflow's autograd does a good job at keeping track where the gradients should flow and thus the differentiable implementation looks almost identical to the classical implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft assignment\n",
    "Tensorflow does not allow direct assignment of array indexes, so we use this trick. For more information go to [bubble-sort.ipynb](bubble-sort.ipynb) or [differentiable-indexed-arrays.ipynb](differentiable-indexed-arrays.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def assign_index(arr, index, element):\n",
    "    arr_shape = tf.shape(arr)\n",
    "    \n",
    "    pos_mask = tf.eye(arr_shape[0])[index]\n",
    "    pos_mask = tf.transpose(tf.expand_dims(pos_mask, 0))\n",
    "    neg_mask = 1 - pos_mask\n",
    "    \n",
    "    tiled_element = tf.reshape(tf.tile(element, [arr_shape[0]]), arr_shape)\n",
    "    \n",
    "    arr = arr * neg_mask + tiled_element * pos_mask\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack push\n",
    "The `stack_push` function is a stateless function. At the time of writing, the Autograph has undefined behaviour if we try to build a stateful implementation of stack like using python class or using closures.\n",
    "\n",
    "The `state` variable has two variables, buffer and index. The buffer is the writable buffer where stack elements are stored. Index points to top of stack + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [2. 2. 2.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def stack_push(state, element):\n",
    "    buffer, index = state\n",
    "    buffer = assign_index(buffer, index, element)\n",
    "    index += 1\n",
    "    \n",
    "    state = (buffer, index)\n",
    "    return state\n",
    "\n",
    "buffer = tf.zeros((3,3), dtype=tf.float32)\n",
    "index = tf.constant(0, dtype=tf.int32)\n",
    "state = (buffer, index)\n",
    "elements = tf.Variable([\n",
    "    [1,1,1],\n",
    "    [2,2,2],\n",
    "    [3,3,3]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    state = stack_push(state, elements[0])\n",
    "    state = stack_push(state, elements[1])\n",
    "    \n",
    "print(state[0])\n",
    "print(state[1])\n",
    "print(tape.gradient(state[0], elements))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack pop\n",
    "For buffer lookup we use the naive approach as described in [differentiable-indexed-arrays.ipynb](differentiable-indexed-arrays.ipynb). We also update the index and return both state and element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 1. 1.]\n",
      " [2. 2. 2.]\n",
      " [3. 3. 3.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [1. 1. 1.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def stack_pop(state):\n",
    "    buffer, index = state\n",
    "    index -= 1\n",
    "    element = buffer[index]\n",
    "    \n",
    "    state = (buffer, index)\n",
    "    return state, element\n",
    "\n",
    "index = tf.constant(3, dtype=tf.int32)\n",
    "buffer = tf.Variable([\n",
    "    [1,1,1],\n",
    "    [2,2,2],\n",
    "    [3,3,3]\n",
    "],dtype=tf.float32)\n",
    "state = (buffer, index)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    ns1, element = stack_pop(state)\n",
    "    ns2, element = stack_pop(ns1)\n",
    "\n",
    "print(ns2[0])\n",
    "print(ns2[1])\n",
    "print(tape.gradient(element, buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy example: Reversing a list\n",
    "Using two stacks, we can reverse a list. The algorithm has two steps\n",
    "* Stack 1 pushes all elements into itself\n",
    "* Stack 1 then pops an element and Stack 2 pushes that element into itself\n",
    "\n",
    "The buffer of Stack 2 is the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[4. 4. 4. 4.]\n",
      " [3. 3. 3. 3.]\n",
      " [2. 2. 2. 2.]\n",
      " [1. 1. 1. 1.]], shape=(4, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def reverse_list(arr):\n",
    "    arr_shape = tf.shape(arr)\n",
    "    arr = tf.unstack(arr)\n",
    "    \n",
    "    buffer1 = tf.zeros(arr_shape, dtype=tf.float32)\n",
    "    index1 = tf.constant(0, dtype=tf.int32)\n",
    "    state1 = (buffer1, index1)\n",
    "    \n",
    "    # Step 1: Push all elements into stack 1\n",
    "    for element in arr:\n",
    "        state1 = stack_push(state1, element)\n",
    "    \n",
    "    buffer2 = tf.zeros(arr_shape, dtype=tf.float32)\n",
    "    index2 = tf.constant(0, dtype=tf.int32)\n",
    "    state2 = (buffer2, index2)\n",
    "    \n",
    "    # Step 2: Transfer all elements to stack 2\n",
    "    for _ in tf.range(arr_shape[0]):\n",
    "        state1, element = stack_pop(state1)\n",
    "        state2 = stack_push(state2, element)\n",
    "    \n",
    "    # Return buffer of stack 2\n",
    "    return state2[0]\n",
    "\n",
    "arr = tf.Variable([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3],\n",
    "    [4,4,4,4],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    new_arr = reverse_list(arr)\n",
    "\n",
    "print(new_arr)\n",
    "print(tape.gradient(new_arr, arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward pass\n",
    "To demonstrate the working of backward pass, we give a reversed target array `reversed_arr` to the algorithm and a learnable `input_arr`. The algorithm must learn the `input_arr` using gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "10.2250357\n",
      "2.75192738\n",
      "0.439089298\n",
      "0.129742727\n",
      "0.0685746\n",
      "0.0525279418\n",
      "0.0155251706\n",
      "0.000200064795\n",
      "0.00172046362\n",
      "[[1 1 1 1]\n",
      " [2 2 2 2]\n",
      " [3 3 3 3]\n",
      " [4 4 4 4]]\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(1e-1)\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_ = reverse_list(x)\n",
    "        loss = tf.nn.l2_loss(y - y_)\n",
    "        \n",
    "    grads = tape.gradient(loss, x)\n",
    "    opt.apply_gradients(zip([grads], [x]))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "input_arr = tf.Variable([\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1]\n",
    "], dtype=tf.float32)\n",
    "reversed_arr = tf.constant([\n",
    "    [4,4,4,4],\n",
    "    [3,3,3,3],\n",
    "    [2,2,2,2],\n",
    "    [1,1,1,1],\n",
    "], dtype=tf.float32)\n",
    "\n",
    "for i in range(100):\n",
    "    loss = train_step(input_arr, reversed_arr)\n",
    "    if i % 10 == 0:\n",
    "        tf.print(loss)\n",
    "tf.print(tf.round(input_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit238183a89ccd4c25acc508071275f29e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
