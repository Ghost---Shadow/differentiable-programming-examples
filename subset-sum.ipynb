{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Sum Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Implementation (Approximate)\n",
    "\n",
    "Let the superset be the following vector and our target be $7.0$.\n",
    "\n",
    "$$\n",
    "target = 7.0\n",
    "$$\n",
    "\n",
    "\\begin{equation*}\n",
    "superset = \n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0 & 3.0 & 4.0 & 5.0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Our goal is to find a mask, such that, the dot product results in the target. Here is an example of a mask that adds up to our target.\n",
    "\n",
    "\\begin{equation*}\n",
    "mask = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 & 1.0 & 1.0 & 0.0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "We can verify that $$ mask \\cdot superset = target $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bistable loss\n",
    "See [boolean-satisfiability.ipynb](boolean-satisfiability.ipynb) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def bistable_loss_fn(x):\n",
    "    a = (x ** 2)\n",
    "    b = (x - 1) ** 2\n",
    "    \n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Loss\n",
    "\n",
    "To force the optimzer to pick boolean like values over minimizing squared difference, we give more weight to the bistable loss.\n",
    "\n",
    "$$ loss_{total} = \\sqrt{(mask \\cdot superset - target) ^ 2} + e^{loss_{bistable}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def total_loss(target, subset_sum, mask):\n",
    "    l2_loss = tf.math.squared_difference(target, subset_sum)\n",
    "    bistable_loss = tf.reduce_sum(bistable_loss_fn(mask))\n",
    "    \n",
    "    return l2_loss + tf.exp(bistable_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def subset_sum_fn(mask, container):\n",
    "    return tf.tensordot(mask, container, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(65.0, shape=(), dtype=float32)\n",
      "[1. 1. 1. 1. 1.]\n",
      "tf.Tensor([16. 32. 48. 64. 80.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([16. 16. 16. 16. 16.], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "container = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "mask = tf.Variable(tf.ones(tf.shape(container)),dtype=tf.float32)\n",
    "target = tf.constant(7.0, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    subset_sum = subset_sum_fn(mask, container)\n",
    "    loss = total_loss(target, subset_sum, mask)\n",
    "\n",
    "print(loss)\n",
    "print(mask.numpy())\n",
    "print(tape.gradient(loss,mask))\n",
    "print(tape.gradient(loss,container))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 65.0 [1. 1. 1. 1. 1.] 15.0\n",
      "1000 1.4589697 [0.49017498 0.4877046  0.4869398  0.48656726 0.48634836] 0.0\n",
      "2000 1.3628395 [0.46271044 0.46523246 0.46614125 0.4666031  0.46688387] 0.0\n",
      "3000 1.3617111 [0.44354406 0.46168292 0.46652192 0.46874067 0.4700129 ] 0.0\n",
      "4000 1.3545029 [0.37869254 0.4552448  0.46950325 0.4751845  0.4782023 ] 0.0\n",
      "5000 1.3024449 [0.1474902  0.44448635 0.48462448 0.4968005  0.5023167 ] 5.0\n",
      "6000 1.2715133 [0.00471203 0.36340576 0.5007747  0.52516717 0.532443  ] 12.0\n",
      "7000 1.2001972 [-0.00185637  0.05919809  0.53151894  0.5876373   0.58854026] 12.0\n",
      "8000 1.182796 [-0.00845798 -0.01663175  0.44833654  0.6654444   0.6089821 ] 9.0\n",
      "9000 1.0536028 [-0.01366616 -0.02505278 -0.02220595  0.90308255  0.70674056] 9.0\n"
     ]
    }
   ],
   "source": [
    "container = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "mask = tf.Variable(tf.ones(tf.shape(container)),dtype=tf.float32)\n",
    "target = tf.constant(7.0, dtype=tf.float32)\n",
    "\n",
    "# opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "for i in range(10000):\n",
    "    with tf.GradientTape() as tape:\n",
    "        subset_sum = subset_sum_fn(mask, container)\n",
    "        loss = total_loss(target, subset_sum, mask)\n",
    "    if i % 1000 == 0:\n",
    "        answer = tf.reduce_sum(tf.round(mask) * container)\n",
    "        print(i, loss.numpy(), mask.numpy(), answer.numpy())\n",
    "    grads = tape.gradient(loss, mask)\n",
    "    opt.apply_gradients(zip([grads], [mask]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "We get the mask\n",
    "\n",
    "\\begin{equation*}\n",
    "mask = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 & 0.0 & 1.0 & 1.0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Which gives the sum of $9.0$ instead of $7.0$.\n",
    "\n",
    "The system seems to be stuck in a local optima. Training further would not improve the results. \n",
    "\n",
    "**Note: Since, our batch size is one. We are training with Gradient Descent instead of Stochastic Gradient Descent. Global Optima is not guaranteed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
