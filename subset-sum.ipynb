{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Sum Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Implementation (Approximate)\n",
    "\n",
    "Let the superset be the following vector and our target be $7.0$.\n",
    "\n",
    "$$\n",
    "target = 7.0\n",
    "$$\n",
    "\n",
    "\\begin{equation*}\n",
    "superset = \n",
    "\\begin{bmatrix}\n",
    "1.0 & 2.0 & 3.0 & 4.0 & 5.0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Our goal is to find a mask, such that, the dot product results in the target. Here is an example of a mask that adds up to our target.\n",
    "\n",
    "\\begin{equation*}\n",
    "mask = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 & 1.0 & 1.0 & 0.0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "We can verify that $$ mask \\cdot superset = target $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bistable loss\n",
    "See [boolean-satisfiability.ipynb](boolean-satisfiability.ipynb) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def bistable_loss_fn(x):\n",
    "    a = (x ** 2)\n",
    "    b = (x - 1) ** 2\n",
    "    \n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Loss\n",
    "\n",
    "To force the optimzer to pick boolean like values over minimizing squared difference, we give more weight to the bistable loss.\n",
    "\n",
    "$$ loss_{total} = \\sqrt{(mask \\cdot superset - target) ^ 2} + e^{loss_{bistable}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def total_loss(target, subset_sum, mask):\n",
    "    l2_loss = tf.math.squared_difference(target, subset_sum)\n",
    "    bistable_loss = tf.reduce_sum(bistable_loss_fn(mask))\n",
    "    \n",
    "    return l2_loss + tf.exp(bistable_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def subset_sum_fn(mask, container):\n",
    "    return tf.tensordot(mask, container, axes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(65.0, shape=(), dtype=float32)\n",
      "[1. 1. 1. 1. 1.]\n",
      "tf.Tensor([16. 32. 48. 64. 80.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([16. 16. 16. 16. 16.], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "container = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "mask = tf.Variable(tf.ones(tf.shape(container)),dtype=tf.float32)\n",
    "target = tf.constant(7.0, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    subset_sum = subset_sum_fn(mask, container)\n",
    "    loss = total_loss(target, subset_sum, mask)\n",
    "\n",
    "print(loss)\n",
    "print(mask.numpy())\n",
    "print(tape.gradient(loss,mask))\n",
    "print(tape.gradient(loss,container))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "mask = tf.Variable(tf.ones(tf.shape(container)),dtype=tf.float32)\n",
    "target = tf.constant(7.0, dtype=tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        subset_sum = subset_sum_fn(mask, container)\n",
    "        loss = total_loss(target, subset_sum, mask)\n",
    "    grads = tape.gradient(loss, mask)\n",
    "    opt.apply_gradients(zip([grads], [mask]))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 65 [0.998999953 0.998999953 0.998999953 0.998999953 0.998999953] 15\n",
      "1000 1.45896971 [0.49006924 0.487600476 0.486836314 0.486464143 0.486245453] 0\n",
      "2000 1.36283946 [0.462699383 0.465229839 0.466141194 0.466604263 0.466885746] 0\n",
      "3000 1.36171114 [0.44351235 0.461678118 0.466522902 0.468744069 0.470017612] 0\n",
      "4000 1.35450292 [0.378572643 0.455236733 0.46950981 0.475195825 0.478215873] 0\n",
      "5000 1.30244493 [0.147182733 0.444465905 0.484647065 0.496831417 0.502350032] 5\n",
      "6000 1.27151334 [0.00469869794 0.363226235 0.50079155 0.525202513 0.532478869] 12\n",
      "7000 1.20019722 [-0.00187286059 0.058930587 0.531536877 0.587706268 0.588588715] 12\n",
      "8000 1.18279672 [-0.00845454726 -0.0166277438 0.448080063 0.665599108 0.60900861] 9\n",
      "9000 1.05360293 [-0.0136801191 -0.0250828899 -0.0223536715 0.903152943 0.706790388] 9\n"
     ]
    }
   ],
   "source": [
    "# opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "for i in range(10000):\n",
    "    loss = train_step()\n",
    "    if i % 1000 == 0:\n",
    "        answer = tf.reduce_sum(tf.round(mask) * container)\n",
    "        tf.print(i, loss, mask, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    "\n",
    "We get the mask\n",
    "\n",
    "\\begin{equation*}\n",
    "mask = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 & 0.0 & 1.0 & 1.0 \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Which gives the sum of $9.0$ instead of $7.0$.\n",
    "\n",
    "The system seems to be stuck in a local optima. Training further would not improve the results. \n",
    "\n",
    "**Note: Since, our batch size is one. We are training with Gradient Descent instead of Stochastic Gradient Descent. Global Optima is not guaranteed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
